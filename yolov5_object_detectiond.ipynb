{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from https://app.roboflow.com/ds/g5RJr8Jk39?key=qZbtqPkhlX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sourav/Documents/fruit object detection/fruit_object_detection/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone YOLOv5 and \n",
    "#!git clone https://github.com/ultralytics/yolov5  # clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sourav/Documents/fruit object detection/yolov5\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'yolov5'\n",
      "/home/sourav/Documents/fruit object detection/yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sourav/Documents/fruit object detection/yolov5\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: ../train/images\n",
      "val: ../valid/images\n",
      "\n",
      "nc: 3\n",
      "names: ['apple', 'banana', 'orange']"
     ]
    }
   ],
   "source": [
    "%cat ../data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=../data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=16, imgsz=415, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
      "fatal: cannot change to '/home/sourav/Documents/fruit': No such file or directory\n",
      "YOLOv5 ðŸš€ 2022-7-5 Python-3.8.10 torch-1.12.0+cu102 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /home/sourav/.config/Ultralytics/Arial.ttf...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 843kB/s]\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:19<00:00, 768kB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 270 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "WARNING: --img-size 415 must be multiple of max stride 32, updating to 416\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/home/sourav/Documents/fruit object detection/train/labels' ima\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/sourav/Documents/fruit object detection/train/labels.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 612/612 [00:02<00:00, 286.75\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/sourav/Documents/fruit object detection/test/labels' images\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/sourav/Documents/fruit object detection/test/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:00<00:00, 132.00it/s\u001b[0m\n",
      "Plotting labels to runs/train/exp2/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.84 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Image sizes 416 train, 416 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/29        0G   0.09067   0.03087   0.03833        13       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@WARNING: NMS time limit 1.260s exceeded\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         89      0.142      0.277      0.191     0.0571\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/29        0G   0.06521   0.03033   0.02888        13       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.134      0.306       0.12     0.0294\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/29        0G   0.06178   0.02823   0.02123         9       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.297      0.366      0.323      0.125\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/29        0G   0.05706   0.02734   0.01527        18       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.485      0.572      0.475      0.126\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/29        0G   0.05695    0.0292    0.0117        22       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.492      0.593      0.528      0.215\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/29        0G   0.04903   0.02677   0.01029         8       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.538      0.626      0.576      0.194\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/29        0G   0.04531    0.0271   0.01005        14       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.491      0.438      0.406      0.138\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/29        0G   0.04337   0.02709  0.008493        18       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.642       0.78      0.737      0.345\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/29        0G   0.04078   0.02563  0.007287        15       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.748      0.684      0.742      0.311\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/29        0G   0.03835   0.02506  0.008436        11       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.656      0.773      0.745       0.34\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/29        0G   0.03682   0.02453   0.00689        14       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99       0.78      0.748      0.813      0.398\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/29        0G   0.03544   0.02521   0.00766        21       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.622       0.82       0.77      0.425\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/29        0G   0.03595   0.02407  0.005992        20       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.664      0.836      0.778      0.384\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/29        0G   0.03493    0.0238  0.007062        19       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.789      0.774      0.795      0.294\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/29        0G   0.03563   0.02336  0.005984        12       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.754      0.751      0.791       0.42\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/29        0G   0.03404   0.02227  0.004611        19       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.805      0.783      0.845      0.499\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/29        0G   0.03322   0.02296  0.005279        14       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.877      0.837      0.885      0.457\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/29        0G   0.03266   0.02241  0.004975        19       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.784      0.836      0.838      0.437\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/29        0G   0.03197   0.02316  0.004113        14       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.806       0.74      0.821      0.411\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/29        0G   0.03227   0.02343  0.004784        19       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.803      0.872      0.869       0.45\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/29        0G    0.0311   0.02226  0.004818        22       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.882      0.823      0.887      0.421\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/29        0G   0.03033   0.02165  0.004204        20       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.795      0.909      0.891      0.457\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/29        0G   0.03003    0.0218  0.003445        18       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.823      0.929      0.912      0.491\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/29        0G   0.02945   0.02288  0.003843        23       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.816      0.864      0.885      0.461\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/29        0G   0.02989   0.02185  0.003723        27       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.846      0.912      0.887      0.406\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     25/29        0G   0.02863   0.02061  0.004272        19       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.834      0.861       0.89      0.461\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     26/29        0G   0.02936   0.02144  0.004042        19       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.835      0.868       0.89      0.464\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     27/29        0G   0.02778   0.02006  0.003452        17       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.856      0.858      0.881      0.463\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     28/29        0G    0.0274   0.01964  0.003122        26       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.822      0.917      0.889      0.481\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     29/29        0G   0.02594   0.01963   0.00324        15       416: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.807      0.914      0.883      0.478\n",
      "\n",
      "30 epochs completed in 3.650 hours.\n",
      "Optimizer stripped from runs/train/exp2/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/exp2/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/exp2/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all         56         99      0.803      0.784      0.845        0.5\n",
      "               apple         56         30      0.656      0.867      0.869      0.608\n",
      "              banana         56         34      0.915      0.588      0.771      0.353\n",
      "              orange         56         35      0.839      0.896      0.895      0.538\n",
      "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n",
      "CPU times: user 5min 59s, sys: 45.6 s, total: 6min 44s\n",
      "Wall time: 3h 41min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python train.py --img 415 --batch 16 --epochs 30 --data ../data.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./runs/train/exp2/weights/best.pt'], source=../test/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "fatal: cannot change to '/home/sourav/Documents/fruit': No such file or directory\n",
      "YOLOv5 ðŸš€ 2022-7-5 Python-3.8.10 torch-1.12.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/56 /home/sourav/Documents/fruit object detection/test/images/apple_77_jpg.rf.7b58ece6e3ed8cd333a2660b3bcb2bb8.jpg: 640x640 6 apples, Done. (0.790s)\n",
      "image 2/56 /home/sourav/Documents/fruit object detection/test/images/apple_78_jpg.rf.a65d3acc908e4fed3c361203d6d5dfa1.jpg: 640x640 Done. (0.813s)\n",
      "image 3/56 /home/sourav/Documents/fruit object detection/test/images/apple_80_jpg.rf.4e3bae71012bcd1909297cedb38b18b3.jpg: 640x640 1 apple, Done. (0.531s)\n",
      "image 4/56 /home/sourav/Documents/fruit object detection/test/images/apple_81_jpg.rf.1abaf37283e1b63b2057368f5f77125e.jpg: 640x640 Done. (0.589s)\n",
      "image 5/56 /home/sourav/Documents/fruit object detection/test/images/apple_82_jpg.rf.afa8a83e89aea23da3237642fcdf9bfe.jpg: 640x640 Done. (0.462s)\n",
      "image 6/56 /home/sourav/Documents/fruit object detection/test/images/apple_83_jpg.rf.811927d61fc02453dd7b940595abf63e.jpg: 640x640 Done. (0.533s)\n",
      "image 7/56 /home/sourav/Documents/fruit object detection/test/images/apple_84_jpg.rf.744446897b4c1186ffae3cf42a941d95.jpg: 640x640 1 apple, Done. (0.576s)\n",
      "image 8/56 /home/sourav/Documents/fruit object detection/test/images/apple_85_jpg.rf.aacd858195def179cd6f31ab95e0eee9.jpg: 640x640 Done. (0.508s)\n",
      "image 9/56 /home/sourav/Documents/fruit object detection/test/images/apple_86_jpg.rf.9df9df5c6bf73ff3f72c47a2df804b13.jpg: 640x640 4 apples, 1 orange, Done. (0.624s)\n",
      "image 10/56 /home/sourav/Documents/fruit object detection/test/images/apple_87_jpg.rf.f7f8c495beab7c4a96379662878addc6.jpg: 640x640 1 apple, Done. (0.583s)\n",
      "image 11/56 /home/sourav/Documents/fruit object detection/test/images/apple_88_jpg.rf.14e3ac707b36f3315f0b2d74e8a21604.jpg: 640x640 1 apple, Done. (0.502s)\n",
      "image 12/56 /home/sourav/Documents/fruit object detection/test/images/apple_89_jpg.rf.9403d5d4e3a6e1719868c356fdf0fb4c.jpg: 640x640 Done. (0.612s)\n",
      "image 13/56 /home/sourav/Documents/fruit object detection/test/images/apple_90_jpg.rf.fcd1562563425ca31deb04415cfe52d5.jpg: 640x640 Done. (0.505s)\n",
      "image 14/56 /home/sourav/Documents/fruit object detection/test/images/apple_91_jpg.rf.47fd23d7f371aaad3fcbc475a9b3b009.jpg: 640x640 Done. (0.497s)\n",
      "image 15/56 /home/sourav/Documents/fruit object detection/test/images/apple_93_jpg.rf.c371714bf86a10273d1be2c8e545d82b.jpg: 640x640 Done. (0.588s)\n",
      "image 16/56 /home/sourav/Documents/fruit object detection/test/images/apple_94_jpg.rf.c5de44352052ea254da42f88d9f865af.jpg: 640x640 5 apples, Done. (0.618s)\n",
      "image 17/56 /home/sourav/Documents/fruit object detection/test/images/apple_95_jpg.rf.0cbf84dd98af6af6071833136671b9ad.jpg: 640x640 3 apples, 1 orange, Done. (0.623s)\n",
      "image 18/56 /home/sourav/Documents/fruit object detection/test/images/banana_77_jpg.rf.90c00656bda6b5d17e088ca1e2adb1dd.jpg: 640x640 Done. (0.593s)\n",
      "image 19/56 /home/sourav/Documents/fruit object detection/test/images/banana_78_jpg.rf.9ffc45d030151b581c2c89575e168c12.jpg: 640x640 Done. (0.695s)\n",
      "image 20/56 /home/sourav/Documents/fruit object detection/test/images/banana_79_jpg.rf.ed2ebdbacc062ad9effc40b9e1721027.jpg: 640x640 Done. (0.506s)\n",
      "image 21/56 /home/sourav/Documents/fruit object detection/test/images/banana_80_jpg.rf.0ae7f829a6fa41f63d75baf2b8760579.jpg: 640x640 Done. (0.510s)\n",
      "image 22/56 /home/sourav/Documents/fruit object detection/test/images/banana_81_jpg.rf.d3d096a94561add52d3805e085e48cbf.jpg: 640x640 Done. (0.535s)\n",
      "image 23/56 /home/sourav/Documents/fruit object detection/test/images/banana_82_jpg.rf.cfdc0f78e250f20959786686172c1160.jpg: 640x640 Done. (0.576s)\n",
      "image 24/56 /home/sourav/Documents/fruit object detection/test/images/banana_83_jpg.rf.84d002f6225e37cbbd0853de881c2d9f.jpg: 640x640 3 apples, Done. (0.589s)\n",
      "image 25/56 /home/sourav/Documents/fruit object detection/test/images/banana_84_jpg.rf.8059344843acb78b7a0cd761aa5ca50d.jpg: 640x640 Done. (0.623s)\n",
      "image 26/56 /home/sourav/Documents/fruit object detection/test/images/banana_85_jpg.rf.1427b11bf8ff2f1afde571a5c6a1effa.jpg: 640x640 1 banana, Done. (0.493s)\n",
      "image 27/56 /home/sourav/Documents/fruit object detection/test/images/banana_86_jpg.rf.267929ed580f21c84ab6ca333eed520b.jpg: 640x640 Done. (0.495s)\n",
      "image 28/56 /home/sourav/Documents/fruit object detection/test/images/banana_88_jpg.rf.cdc3f2ac9ee339dd337fd688f6667a74.jpg: 640x640 1 banana, Done. (0.532s)\n",
      "image 29/56 /home/sourav/Documents/fruit object detection/test/images/banana_89_jpg.rf.5b9dce53649f0ac07d86aebacd43b8df.jpg: 640x640 Done. (0.542s)\n",
      "image 30/56 /home/sourav/Documents/fruit object detection/test/images/banana_90_jpg.rf.ee2426f380fbc98da50712683eddd051.jpg: 640x640 1 banana, Done. (0.581s)\n",
      "image 31/56 /home/sourav/Documents/fruit object detection/test/images/banana_91_jpg.rf.20e057b35a97f0c06e8b01992b81c948.jpg: 640x640 1 orange, Done. (0.580s)\n",
      "image 32/56 /home/sourav/Documents/fruit object detection/test/images/banana_92_jpg.rf.4a0c601eddd7bfdc5b00da02584636bb.jpg: 640x640 Done. (0.463s)\n",
      "image 33/56 /home/sourav/Documents/fruit object detection/test/images/banana_93_jpg.rf.388dd1e5c911d6d479889f230e19abed.jpg: 640x640 2 bananas, Done. (0.489s)\n",
      "image 34/56 /home/sourav/Documents/fruit object detection/test/images/banana_94_jpg.rf.53ae63217c191cfa3c691b2bdf4ce879.jpg: 640x640 Done. (0.533s)\n",
      "image 35/56 /home/sourav/Documents/fruit object detection/test/images/mixed_21_jpg.rf.6bfd8e2a0e0c828ed4ede8b5e79b1e39.jpg: 640x640 5 oranges, Done. (0.511s)\n",
      "image 36/56 /home/sourav/Documents/fruit object detection/test/images/mixed_22_jpg.rf.aeab7209c6d562b713f8f9965658c767.jpg: 640x640 2 apples, 1 banana, Done. (0.506s)\n",
      "image 37/56 /home/sourav/Documents/fruit object detection/test/images/mixed_23_jpg.rf.8f6658da3d396f33cd2919f045a009a4.jpg: 640x640 18 apples, 1 banana, 2 oranges, Done. (0.538s)\n",
      "image 38/56 /home/sourav/Documents/fruit object detection/test/images/mixed_24_jpg.rf.fca9cd7befde245300489fd03b3b77dd.jpg: 640x640 1 apple, Done. (0.486s)\n",
      "image 39/56 /home/sourav/Documents/fruit object detection/test/images/mixed_25_jpg.rf.0cdf1ff4a741de413d9aaa854ba6864e.jpg: 640x640 1 apple, 1 orange, Done. (0.491s)\n",
      "image 40/56 /home/sourav/Documents/fruit object detection/test/images/orange_77_jpg.rf.74b93aeb16b030c3aa74856942f7d0dc.jpg: 640x640 4 oranges, Done. (0.460s)\n",
      "image 41/56 /home/sourav/Documents/fruit object detection/test/images/orange_78_jpg.rf.9e1fcd312e2606f6d509e97a6c71ee59.jpg: 640x640 Done. (0.487s)\n",
      "image 42/56 /home/sourav/Documents/fruit object detection/test/images/orange_79_jpg.rf.38e9f88031ab3c616dcc0c8bc88758fd.jpg: 640x640 4 oranges, Done. (0.541s)\n",
      "image 43/56 /home/sourav/Documents/fruit object detection/test/images/orange_80_jpg.rf.942dfdf13258f0b54b6b5427dced6ab1.jpg: 640x640 Done. (0.593s)\n",
      "image 44/56 /home/sourav/Documents/fruit object detection/test/images/orange_81_jpg.rf.bad441aa18a83b3d7cfd642f0b64fd98.jpg: 640x640 2 oranges, Done. (0.573s)\n",
      "image 45/56 /home/sourav/Documents/fruit object detection/test/images/orange_82_jpg.rf.1062c152579f730d1ba3a5a854673306.jpg: 640x640 1 orange, Done. (0.579s)\n",
      "image 46/56 /home/sourav/Documents/fruit object detection/test/images/orange_83_jpg.rf.cd5415603b90f370c13227c77f685a12.jpg: 640x640 Done. (0.588s)\n",
      "image 47/56 /home/sourav/Documents/fruit object detection/test/images/orange_84_jpg.rf.4e2af45010c391dc2e991134f6b566ba.jpg: 640x640 Done. (0.548s)\n",
      "image 48/56 /home/sourav/Documents/fruit object detection/test/images/orange_85_jpg.rf.d8cf96f76ce12688d2fe2fdf7be79d7f.jpg: 640x640 Done. (0.531s)\n",
      "image 49/56 /home/sourav/Documents/fruit object detection/test/images/orange_86_jpg.rf.d170add765fd717b6d03d7730680ce06.jpg: 640x640 2 oranges, Done. (0.564s)\n",
      "image 50/56 /home/sourav/Documents/fruit object detection/test/images/orange_87_jpg.rf.b2404499ab4f9de70ec00e21c877865c.jpg: 640x640 2 oranges, Done. (0.637s)\n",
      "image 51/56 /home/sourav/Documents/fruit object detection/test/images/orange_89_jpg.rf.32d4107cb76719be40f67ef3fe769128.jpg: 640x640 2 oranges, Done. (0.462s)\n",
      "image 52/56 /home/sourav/Documents/fruit object detection/test/images/orange_90_jpg.rf.6c0f330835713244bb3484f982f9b367.jpg: 640x640 5 oranges, Done. (0.518s)\n",
      "image 53/56 /home/sourav/Documents/fruit object detection/test/images/orange_91_jpg.rf.52abc266654dc749140549d28d3ef2e6.jpg: 640x640 Done. (0.486s)\n",
      "image 54/56 /home/sourav/Documents/fruit object detection/test/images/orange_92_jpg.rf.5895f2c2ada78082280d86f6801ea7f9.jpg: 640x640 Done. (0.473s)\n",
      "image 55/56 /home/sourav/Documents/fruit object detection/test/images/orange_93_jpg.rf.81820a0025e48fa1f4ad68b5db8e6c3a.jpg: 640x640 2 oranges, Done. (0.534s)\n",
      "image 56/56 /home/sourav/Documents/fruit object detection/test/images/orange_95_jpg.rf.6f0bb91203abf708d76d2da57b9ff820.jpg: 640x640 Done. (0.530s)\n",
      "Speed: 4.2ms pre-process, 552.2ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!python detect.py --source runs/train/exp/testimg.jpg --weights runs/train/exp/weights/best.pt --conf 0.25\n",
    "\n",
    "!python detect.py --source \"../test/images\" --weights \"./runs/train/exp2/weights/best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sourav/Documents/fruit object detection/yolov5\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model for a new image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./runs/train/exp2/weights/best.pt'], source=../images.jpeg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "fatal: cannot change to '/home/sourav/Documents/fruit': No such file or directory\n",
      "YOLOv5 ðŸš€ 2022-7-5 Python-3.8.10 torch-1.12.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/1 /home/sourav/Documents/fruit object detection/images.jpeg: 384x640 7 apples, Done. (0.445s)\n",
      "Speed: 2.7ms pre-process, 444.7ms inference, 2.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --source \"../images.jpeg\" --weights \"./runs/train/exp2/weights/best.pt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('fruit_object_detection': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5feffc0a9dd22633414d4d2664381aad0c478be0b0e0c721fcb9ac38d9dd5c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
